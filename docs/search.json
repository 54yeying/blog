[
  {
    "objectID": "posts/started/index.html",
    "href": "posts/started/index.html",
    "title": "你好, Quarto",
    "section": "",
    "text": "今天把博客从Hugo迁移到Quarto。在这一天也有了自己人生未来的新计划。\n                             | |\n__  ___   _  __ _ _ __    ___| |__  _   _\n\\ \\/ / | | |/ _` | '_ \\  / __| '_ \\| | | |\n >  <| |_| | (_| | | | | \\__ \\ | | | |_| |\n/_/\\_\\\\__,_|\\__,_|_| |_| |___/_| |_|\\__,_|\n__   __    __   ___\n\\ \\ / /    \\ \\ / (_)\n \\ V /___   \\ V / _ _ __   __ _\n  \\ // _ \\   \\ / | | '_ \\ / _` |\n  | |  __/   | | | | | | | (_| |\n  \\_/\\___|   \\_/ |_|_| |_|\\__, |\n                           __/ |\n                          |___/"
  },
  {
    "objectID": "posts/kubernetes_the_hard_way/index.html",
    "href": "posts/kubernetes_the_hard_way/index.html",
    "title": "Kubernetes the hard way",
    "section": "",
    "text": "服务器说明\n\nKubernets Version\n\n\nv1.22.15\n\n\n节点要求\n\n\n节点数 >= 3台\nCPUs >= 2\nMemory >= 2G\n\n\n修改时区 有的系统时区不匹配，需要修改\n\ntimedatectl set-timezone Asia/Shanghai\n\n环境说明\n\n\n\n\n系统类型\nIP地址\n节点角色\nCPU\nMemory\nHostname\n\n\n\n\nCentOS-7.9\n192.168.200.11\nmaster\n>=2\n>=2G\ncluster1\n\n\nCentOS-7.9\n192.168.200.22\nmaster,worker\n>=2\n>=2G\ncuster2\n\n\nCentOS-7.9\n192.168.200.33\nworker\n>=2\n>=2G\ncluster3\n\n\n\n\n使用Vagrant搭建虚拟机节点\n\n\nVagrant: latest\nVirtualBox: 7.0\nvagrant-vbguest: 0.21 (挂载host和guest同步目录)\n\nvagrant plugin install vagrant-vbguest --plugin-version 0.21\nVagrantfile配置文件如下:\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nnodes = [\n  {\n    :name => \"cluster1\",\n    :eth1 => \"192.168.200.11\",\n    :mem => \"4096\",\n    :cpu => \"2\"\n  },\n  {\n    :name => \"cluster2\",\n    :eth1 => \"192.168.200.22\",\n    :mem => \"4096\",\n    :cpu => \"2\"\n  },\n  {\n    :name => \"cluster3\",\n    :eth1 => \"192.168.200.33\",\n    :mem => \"4096\",\n    :cpu => \"2\"\n  },\n]\n\n\nVagrant.configure(\"2\") do |config|\n  # Every Vagrant development environment requires a box.\n  config.vm.box = \"centos/7\"\n\n  nodes.each do |opts|\n    config.vm.define opts[:name] do |config|\n      config.vm.hostname = opts[:name]\n\n      config.vm.provider \"virtualbox\" do |v|\n        v.customize [\"modifyvm\", :id, \"--memory\", opts[:mem]]\n        v.customize [\"modifyvm\", :id, \"--cpus\", opts[:cpu]]\n      end\n\n      #config.ssh.username = \"root\"\n      #config.ssh.private_key_path = \"/Users/jinpeng.d/.ssh/id_rsa\"\n      config.vm.synced_folder \"../share\", \"/vagrant_data\"\n\n      config.vm.network :public_network, ip: opts[:eth1]\n      config.vm.synced_folder \"../share\", \"/vagrant_data\"\n\n    end\n  end\nend\n\n\n系统设置(所有节点)\n\n所有操作需要root权限\nhostname (/etc/hosts)\n安装依赖包\n\nyum update -y\nyum install -y socat conntrack ipvsadm ipset jq sysstat curl iptables libseccomp yum-utils\n\n关闭防火墙,selinux, swap,重置 iptables\n\n# 1. 关闭selinux\nsetenforce 0\nsed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config\n# 2. 关闭防火墙\nsystemctl stop firewalld && systemctl disable firewalld\n\n# 3. 设置ipttables规则\niptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT\n\n# 4. 关闭swap\nvi /etc/fstab\n# 永久禁用注释掉swap\n#/swapfile none swap defaults 0 0\n# 临时禁用\nswapoff -a\n# 这里两者都用，临时修改可以即时生效，不用重启，永久禁用防止重启后不生效\n\n# 5. 关闭dnsmasq(否则无法解析域名)\nservice dnsmasq stop && systemctl disable dnsmasq\n\nkubernetes参数设置\n\ncat > /etc/sysctl.d/kubernetes.conf <<EOF\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_nonlocal_bind = 1\nnet.ipv4.ip_forward = 1\nvm.swappiness = 0\nvm.overcommit_memory = 1\nEOF\n\n# 生效文件\nsysctl -p /etc/sysctl.d/kubernetes.conf\n\n配置免密登录 选择其中一个节点，或者一个单独的机器生成ssh公秘钥对，把公钥放在k8s所有节点服务器上\n\n# 生成公秘钥对, 如果没有可用的\nssh-keygen -t rsa\n\n# 查看公钥内容\ncat ~/.ssh/id_rsa.pub\n\n# 每一台节点机器上配置\necho \"<pubkey content>\" >> ~/.ssh/authorized_keys\n\n配置IP映射(每个节点)\n\ncat > /etc/hosts <<EOF\n192.168.200.11 cluster1\n192.168.200.22 cluster2\n192.168.200.33 cluster3\nEOF\n\n下载k8s组件包\n\nexport VERSION=v1.22.15\n\n# 下载master节点组件\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-apiserver\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-controller-manager\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-scheduler\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubectl\n\n# 下载worker节点组件\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kube-proxy\nwget https://storage.googleapis.com/kubernetes-release/release/${VERSION}/bin/linux/amd64/kubelet\n\n# 下载etcd组件\nwget https://github.com/etcd-io/etcd/releases/download/v3.4.10/etcd-v3.4.10-linux-amd64.tar.gz\ntar -xvf etcd-v3.4.10-linux-amd64.tar.gz\nmv etcd-v3.4.10-linux-amd64/etcd* .\nrm -fr etcd-v3.4.10-linux-amd64*\n\n分发软件包\n\n# 把master相关组件分发到master节点\nMASTERS=(cluster1 cluster2)\nfor instance in ${MASTERS[@]}; do\n  scp kube-apiserver kube-controller-manager kube-scheduler kubectl root@${instance}:/usr/local/bin/\ndone\n\n# 把worker先关组件分发到worker节点\nWORKERS=(cluster2 cluster3)\nfor instance in ${WORKERS[@]}; do\n  scp kubelet kube-proxy root@${instance}:/usr/local/bin/\ndone\n\n# 把etcd组件分发到etcd节点\nETCDS=(cluster1 cluster2 cluster3)\nfor instance in ${ETCDS[@]}; do\n  scp etcd etcdctl root@${instance}:/usr/local/bin/\ndone\n\n\n生成证书\n\n准备工作\n\n\n安装cfssl\nwget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfssl\nwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljson\nchmod +x /usr/local/bin/cfssl*\nkubernetes集群组件\n\n服务端组件:\n\nkube-apiserver\nkube-controller-manager\nkube-scheduler\nkubectl\ncontainer runtime(containerd)\n\n客户端组件:\n\nkubelet\nkube-proxy\ncontainer runtime(containerd)\n\n\n\n\n生成根证书 根证书是集群所有节点共享的，只要创建一个CA证书，后续创建的所有证书都由它签名。 在可以登录到所有节点的控制台机器上创建pki目录存放证书\n\n\n根证书配置文件创建\ncat > ca-config.json <<EOF\n{\n  \"signing\": {\n    \"default\": {\n      \"expiry\": \"876000h\"\n    },\n    \"profiles\": {\n      \"kubernetes\": {\n        \"usages\": [\"signing\", \"key encipherment\", \"server auth\", \"client auth\"],\n        \"expiry\": \"876000h\"\n      }\n    }\n  }\n}\nEOF\n\ncat > ca-csr.json <<EOF\n{\n  \"CN\": \"Kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"US\",\n      \"L\": \"Portland\",\n      \"O\": \"Kubernetes\",\n      \"OU\": \"CA\",\n      \"ST\": \"Oregon\"\n    }\n  ]\n}\nEOF\n生成证书和密钥\n\n生成证书和私钥, ca.pem是证书, ca-key.pem是证书私钥 bash   cfssl gencert -initca ca-csr.json | cfssljson -bare ca 输出文件:\n\nca.pem\nca.csr\nca-key.pem\n\n\nadmin客户端证书\n\n\nadmin客户端证书配置文件\ncat > admin-csr.json <<EOF\n{\n  \"CN\": \"admin\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"system:masters\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成admin客户端证书和私钥(基于根证书和私钥以及证书配置文件)\ncfssl gencert \\\n  -ca=ca.pem \\\n  -ca-key=ca-key.pem \\\n  -config=ca-config.json \\\n  -profile=kubernetes \\\n  admin-csr.json | cfssljson -bare admin\n输出文件:\n\nadmin.csr\nadmin.pem\nadmin-key.pem\n\n\n\nkubelet客户端证书 kubernetes使用一种称为Node Authorizer的专用授权模式来授权kubernetes发出的API请求。 kubelet使用将其标识为system:nodes组中的凭据,其用户名为system: node:nodeName, 接下来给每个Worker节点生成证书。\n\n\n生成kubelet客户端证书配置文件\n\n设置Worker节点列表\n\nWORKERS=(cluster2 cluster3)\nWORKER_IPS=(192.168.200.22 192.168.200.33)\nfor ((i=0;i<${#WORKERS[@]};i++)); do\n  cat > ${WORKERS[$i]}-csr.json <<EOF\n  {\n    \"CN\": \"system:node:${WORKERS[$i]}\",\n    \"key\": {\n      \"algo\": \"rsa\",\n      \"size\": 2048\n    },\n    \"names\": [\n      {\n        \"C\": \"CN\",\n        \"L\": \"Beijing\",\n        \"O\": \"system:nodes\",\n        \"OU\": \"seven\",\n        \"ST\": \"Beijing\"\n      }\n    ]\n  }\nEOF\ndone\n生成kubelet客户端证书和密钥\nfor ((i=0;i<${#WORKERS[@]};i++)); do\n  cfssl gencert \\\n    -ca=ca.pem \\\n    -ca-key=ca-key.pem \\\n    -config=ca-config.json \\\n    -hostname=${WORKERS[$i]},${WORKER_IPS[$i]} \\\n    -profile=kubernetes \\\n    ${WORKERS[$i]}-csr.json | cfssljson -bare ${WORKERS[$i]}\ndone\n输出文件:\n\n{worker-node-name}.csr\n{worker-node-name}.pem\n{worker-node-name}-key.pem\n\n\n\nkube-controller-manager客户端证书\n\n\nkube-controller-manager客户端证书配置文件\n\ncat > kube-controller-manager-csr.json <<EOF\n{\n    \"CN\": \"system:kube-controller-manager\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n      {\n        \"C\": \"CN\",\n        \"ST\": \"BeiJing\",\n        \"L\": \"BeiJing\",\n        \"O\": \"system:kube-controller-manager\",\n        \"OU\": \"seven\"\n      }\n    ]\n}\nEOF\n\n生成kube-controller-manager客户端证书\n\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nkube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n输出文件: + kube-controller-manager.csr + kube-controller-manager.pem + kube-controller-manager-key.pem\n\nkube-proxy客户端证书\n\n\nkube-proxy客户端证书配置文件\ncat > kube-proxy-csr.json <<EOF\n{\n  \"CN\": \"system:kube-proxy\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成kube-proxy客户端证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nkube-proxy-csr.json | cfssljson -bare kube-proxy\n输出文件:\n\nkube-proxy.csr\nkube-proxy.pem\nkube-proxy-key.pem\n\n\n\nkube-scheduler客户端证书\n\n\nkube-scheduler客户端证书配置\ncat > kube-scheduler-csr.json <<EOF\n{\n    \"CN\": \"system:kube-scheduler\",\n    \"key\": {\n        \"algo\": \"rsa\",\n        \"size\": 2048\n    },\n    \"names\": [\n      {\n        \"C\": \"CN\",\n        \"ST\": \"BeiJing\",\n        \"L\": \"BeiJing\",\n        \"O\": \"system:kube-scheduler\",\n        \"OU\": \"seven\"\n      }\n    ]\n}\nEOF\n生成kube-scheduler客户端证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nkube-scheduler-csr.json | cfssljson -bare kube-scheduler\n输出文件:\n\nkube-scheduler.csr\nkube-scheduler.pem\nkube-scheduler-key.pem\n\n\n\nkube-apiserver服务端证书\n\n\nkube-apiserver服务端证书配置\ncat > kubernetes-csr.json <<EOF\n{\n  \"CN\": \"kubernetes\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成kube-apiserver服务端证书 服务端证书与客户端证书不同:\n\n客户端证书需要通过一个名字或者IP去访问服务端，所以证书需要包含客户端所访问的名字或IP，用以客户端验证\n指定可能作为master的节点服务地址\napiserver的 service ip地址(一般是svc网段的第一个ip)\n所有master内网IP和公网IP，逗号分割，(可以把所有节点写上，防止变换master节点)\n\nKUBERNETES_SVC_IP=\"10.233.0.1\"\nMASTER_IPS=\"192.168.200.11,192.168.200.22,192.168.200.33\"\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-hostname=${KUBERNETES_SVC_IP},${MASTER_IPS},127.0.0.1,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster,kubernetes.svc.cluster.local \\\n-profile=kubernetes \\\nkubernetes-csr.json | cfssljson -bare kubernetes\n输出文件:\n\nkubernetes.csr\nkubernetes.pem\nkubernetes-key.pem\n\n\n\nService Account证书\n\n\n配置文件\ncat > service-account-csr.json <<EOF\n{\n  \"CN\": \"service-accounts\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nservice-account-csr.json | cfssljson -bare service-account\n输出文件:\n\nservice-account.csr\nservice-account.pem\nservice-account-key.pem\n\n\n\nproxy-client证书\n\n\n配置文件\ncat > proxy-client-csr.json <<EOF\n{\n  \"CN\": \"aggregator\",\n  \"key\": {\n    \"algo\": \"rsa\",\n    \"size\": 2048\n  },\n  \"names\": [\n    {\n      \"C\": \"CN\",\n      \"ST\": \"BeiJing\",\n      \"L\": \"BeiJing\",\n      \"O\": \"k8s\",\n      \"OU\": \"seven\"\n    }\n  ]\n}\nEOF\n生成证书\ncfssl gencert \\\n-ca=ca.pem \\\n-ca-key=ca-key.pem \\\n-config=ca-config.json \\\n-profile=kubernetes \\\nproxy-client-csr.json | cfssljson -bare proxy-client\n输出文件:\n\nproxy-client.csr\nproxy-client.pem\nproxy-client-key.pem\n\n\n\n分发客户端，服务端证书\n\n\n分发Worker节点需要的证书和私钥\n\n每个Worker节点证书和密钥\n\nWORKERS=(\"cluster2\" \"cluster3\")\nfor instance in ${WORKERS[@]}; do\n  scp ca.pem ${instance}-key.pem ${instance}.pem root@${instance}:~/\ndone\n分发Master节点需要的证书和私钥\n\n根证书和密钥(ca*.pem)\nkube-apiserver证书和密钥(kubenetes*.pem)\nservice-account证书和密钥(service-account*.pem)\nproxy-client证书和密钥(proxy-client*.pem)\n\nMASTER_IPS=(cluster1 cluster2)\nfor instance in ${MASTER_IPS[@]}; do\n  scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\\n    service-account-key.pem service-account.pem proxy-client.pem proxy-client-key.pem root@${instance}:~/\ndone\n\n\n\nkubernetes各组件的认证配置\nkubernetes的认证配置文件，称为kubeconfigs，用于让kubernetes的客户端定位 kube-apiserver并通过apiserver的安全认证。\n\ncontroller-manager\nkubelet\nkube-proxy\nscheduler\nadmin user\n\n\n为kubelet生成kubeconfigs(每个Worker节点)\n\nWORKERS=(\"cluster2\" \"cluster3\")\nfor instance in ${WORKERS[@]}; do\n  kubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://127.0.0.1:6443 \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config set-credentials system:node:${instance} \\\n    --client-certificate=${instance}.pem \\\n    --client-key=${instance}-key.pem \\\n    --embed-certs=true \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config set-context default \\\n    --cluster=kubernetes \\\n    --user=system:node:${instance} \\\n    --kubeconfig=${instance}.kubeconfig\n\n  kubectl config use-context default --kubeconfig=${instance}.kubeconfig\ndone\n\n为kube-proxy生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://127.0.0.1:6443 \\\n    --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-credentials system:kube-proxy \\\n  --client-certificate=kube-proxy.pem \\\n  --client-key=kube-proxy-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-proxy \\\n  --kubeconfig=kube-proxy.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-proxy.kubeconfig\n\n为kube-controller-manager生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=ca.pem \\\n  --embed-certs=true \\\n  --server=https://127.0.0.1:6443 \\\n  --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-credentials system:kube-controller-manager \\\n  --client-certificate=kube-controller-manager.pem \\\n  --client-key=kube-controller-manager-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-controller-manager \\\n  --kubeconfig=kube-controller-manager.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig\n\n为kube-scheduler生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=ca.pem \\\n  --embed-certs=true \\\n  --server=https://127.0.0.1:6443 \\\n  --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-credentials system:kube-scheduler \\\n  --client-certificate=kube-scheduler.pem \\\n  --client-key=kube-scheduler-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=system:kube-scheduler \\\n  --kubeconfig=kube-scheduler.kubeconfig\n\nkubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig\n\n为admin用户生成kubeconfigs\n\nkubectl config set-cluster kubernetes \\\n  --certificate-authority=ca.pem \\\n  --embed-certs=true \\\n  --server=https://127.0.0.1:6443 \\\n  --kubeconfig=admin.kubeconfig\n\nkubectl config set-credentials admin \\\n  --client-certificate=admin.pem \\\n  --client-key=admin-key.pem \\\n  --embed-certs=true \\\n  --kubeconfig=admin.kubeconfig\n\nkubectl config set-context default \\\n  --cluster=kubernetes \\\n  --user=admin \\\n  --kubeconfig=admin.kubeconfig\n\nkubectl config use-context default --kubeconfig=admin.kubeconfig\n\n分发kubeconfigs配置文件\n\n\nkubelet和kube-proxy的kubeconfigs分发到Worker节点\n\nWORKERS=(\"cluster2\" \"cluster3\")\nfor instance in ${WORKERS[@]}; do\n  scp ${instance}.kubeconfig kube-proxy.kubeconfig ${instance}:~/\ndone\n\nkube-controller-manager和kube-scheduler的kubeconfigs分发到Master节点\n\nMASTERS=(\"cluster1\" \"cluster2\")\nfor instance in ${MASTERS[@]}; do\n  scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig ${instance}:~/\ndone\n\n\n部署ETCD集群\n\n配置etcd证书文件\n\nmkdir -p /etc/etcd /var/lib/etcd\nchmod 700 /var/lib/etcd\ncp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/\n\n配置etcd.service文件\n\n\nECCD_IP: 为当前ectd存储服务器的IP\n\nETCD_NAME=$(hostname -s)\nETCD_IP=192.168.200.11\nETCD_NAMES=(cluster1 cluster2 cluster3)\nETCD_IPS=(192.168.200.11 192.168.200.22 192.168.200.33)\ncat <<EOF > /etc/systemd/system/etcd.service\n[Unit]\nDescription=etcd\nDocumentation=https://github.com/coreos\n\n[Service]\nType=notify\nExecStart=/usr/local/bin/etcd \\\\\n  --name ${ETCD_NAME} \\\\\n  --cert-file=/etc/etcd/kubernetes.pem \\\\\n  --key-file=/etc/etcd/kubernetes-key.pem \\\\\n  --peer-cert-file=/etc/etcd/kubernetes.pem \\\\\n  --peer-key-file=/etc/etcd/kubernetes-key.pem \\\\\n  --trusted-ca-file=/etc/etcd/ca.pem \\\\\n  --peer-trusted-ca-file=/etc/etcd/ca.pem \\\\\n  --peer-client-cert-auth \\\\\n  --client-cert-auth \\\\\n  --initial-advertise-peer-urls https://${ETCD_IP}:2380 \\\\\n  --listen-peer-urls https://${ETCD_IP}:2380 \\\\\n  --listen-client-urls https://${ETCD_IP}:2379,https://127.0.0.1:2379 \\\\\n  --advertise-client-urls https://${ETCD_IP}:2379 \\\\\n  --initial-cluster-token etcd-cluster-0 \\\\\n  --initial-cluster ${ETCD_NAMES[0]}=https://${ETCD_IPS[0]}:2380,${ETCD_NAMES[1]}=https://${ETCD_IPS[1]}:2380,${ETCD_NAMES[2]}=https://${ETCD_IPS[2]}:2380 \\\\\n  --initial-cluster-state new \\\\\n  --data-dir=/var/lib/etcd\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n启动etcd集群\n\nsystemctl daemon-reload && systemctl enable etcd && systemctl start etcd\n\n验证etcd集群\n\nETCDCTL_API=3 etcdctl member list \\\n--endpoints=https://127.0.0.1:2379 \\\n--cacert=/etc/etcd/ca.pem \\\n--cert=/etc/etcd/kubernetes.pem \\\n--key=/etc/etcd/kubernetes-key.pem\n成功输出结果如下(不同机器不同):\n87db4208f2c1d75, started, cluster2, https://192.168.200.22:2380, https://192.168.200.22:2379, false\n95c5677668e390bf, started, cluster1, https://192.168.200.11:2380, https://192.168.200.11:2379, false\nd728cb204dcd87a3, started, cluster3, https://192.168.200.33:2380, https://192.168.200.33:2379, false\n\n\n部署kubernetes控制面板\n每个组件有多个点保证高可用。 我们在cluster1和cluster2上部署kube-apiserver,kube-contronller-manager,kube-scheduler\n\n配置API Server(每个master节点都需要配置)\n\nmkdir -p /etc/kubernetes/ssl\n\nmv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\\n  service-account-key.pem service-account.pem \\\n  proxy-client.pem proxy-client-key.pem \\\n  /etc/kubernetes/ssl\n\nIP=192.168.200.11\nAPISERVER_COUNT=2\nETCD_ENDPOINTS=(192.168.200.11 192.168.200.22 192.168.200.33)\n\ncat <<EOF > /etc/systemd/system/kube-apiserver.service\n[Unit]\nDescription=Kubernetes API Server\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-apiserver \\\\\n  --advertise-address=${IP} \\\\\n  --allow-privileged=true \\\\\n  --apiserver-count=${APISERVER_COUNT} \\\\\n  --audit-log-maxage=30 \\\\\n  --audit-log-maxbackup=3 \\\\\n  --audit-log-maxsize=100 \\\\\n  --audit-log-path=/var/log/audit.log \\\\\n  --authorization-mode=Node,RBAC \\\\\n  --bind-address=0.0.0.0 \\\\\n  --client-ca-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\\n  --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\\\\n  --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\\\\n  --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\\\\n  --etcd-servers=https://${ETCD_ENDPOINTS[0]}:2379,https://${ETCD_ENDPOINTS[1]}:2379,https://${ETCD_ENDPOINTS[2]}:2379 \\\\\n  --event-ttl=1h \\\\\n  --kubelet-certificate-authority=/etc/kubernetes/ssl/ca.pem \\\\\n  --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\\\\n  --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\\\\n  --service-account-issuer=api \\\\\n  --service-account-key-file=/etc/kubernetes/ssl/service-account.pem \\\\\n  --service-account-signing-key-file=/etc/kubernetes/ssl/service-account-key.pem \\\\\n  --api-audiences=api,vault,factors \\\\\n  --service-cluster-ip-range=10.233.0.0/16 \\\\\n  --service-node-port-range=30000-32767 \\\\\n  --proxy-client-cert-file=/etc/kubernetes/ssl/proxy-client.pem \\\\\n  --proxy-client-key-file=/etc/kubernetes/ssl/proxy-client-key.pem \\\\\n  --runtime-config=api/all=true \\\\\n  --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --requestheader-allowed-names=aggregator \\\\\n  --requestheader-extra-headers-prefix=X-Remote-Extra- \\\\\n  --requestheader-group-headers=X-Remote-Group \\\\\n  --requestheader-username-headers=X-Remote-User \\\\\n  --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\\\\n  --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\\\\n  --v=1\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n配置kube-controller-manager\n\nmv kube-controller-manager.kubeconfig /etc/kubernetes/\n\ncat <<EOF > /etc/systemd/system/kube-controller-manager.service\n[Unit]\nDescription=Kubernetes Controller Manager\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-controller-manager \\\\\n  --bind-address=0.0.0.0 \\\\\n  --cluster-cidr=10.200.0.0/16 \\\\\n  --cluster-name=kubernetes \\\\\n  --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\\\\n  --cluster-signing-duration=876000h0m0s \\\\\n  --kubeconfig=/etc/kubernetes/kube-controller-manager.kubeconfig \\\\\n  --leader-elect=true \\\\\n  --root-ca-file=/etc/kubernetes/ssl/ca.pem \\\\\n  --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem \\\\\n  --service-cluster-ip-range=10.233.0.0/16 \\\\\n  --use-service-account-credentials=true \\\\\n  --v=1\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n配置kube-scheduler\n\nmv kube-scheduler.kubeconfig /etc/kubernetes\n\ncat <<EOF > /etc/systemd/system/kube-scheduler.service\n[Unit]\nDescription=Kubernetes Scheduler\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-scheduler \\\\\n  --authentication-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\\n  --authorization-kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\\n  --kubeconfig=/etc/kubernetes/kube-scheduler.kubeconfig \\\\\n  --leader-elect=true \\\\\n  --bind-address=0.0.0.0 \\\\\n  --port=0 \\\\\n  --v=1\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n启动服务\n\nsystemctl daemon-reload\nsystemctl enable kube-apiserver\nsystemctl enable kube-controller-manager\nsystemctl enable kube-scheduler\nsystemctl start kube-apiserver\nsystemctl start kube-controller-manager\nsystemctl start kube-scheduler\n\n服务验证\n\nnetstat -ntlp\n正常输出如下:\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      18829/sshd\ntcp        0      0 127.0.0.1:25            0.0.0.0:*               LISTEN      853/master\ntcp        0      0 192.168.200.11:2379     0.0.0.0:*               LISTEN      30516/etcd\ntcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      30516/etcd\ntcp        0      0 192.168.200.11:2380     0.0.0.0:*               LISTEN      30516/etcd\ntcp6       0      0 ::1:25                  :::*                    LISTEN      853/master\ntcp6       0      0 :::6443                 :::*                    LISTEN      30651/kube-apiserve\ntcp6       0      0 :::10257                :::*                    LISTEN      30666/kube-controll\ntcp6       0      0 :::10259                :::*                    LISTEN      30679/kube-schedule\n\n配置kubectl kubectl用来管理kubernetes集群的客户端工具。\n\nmkdir ~/.kube/\nmv ~/admin.kubeconfig ~/.kube/config\nkubectl get nodes\n授权apiserver调用kubelet API,在执行kubectl exec/run/logs时apiserver会转发到kubelet\nkubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes\n\n\nDeploy kubernetes Worker Node\n每个节点上都会部署:\n\nkubelet\nkube-proxy\ncontainer runtime\ncni\nnginx-proxy\n\n\nContainer Runtine(containerd)\n\nVERSION=1.4.3\nwget https://github.com/containerd/containerd/releases/download/v${VERSION}/cri-containerd-cni-${VERSION}-linux-amd64.tar.gz\ntar -xvf cri-containerd-cni-${VERSION}-linux-amd64.tar.gz\ncp etc/crictl.yaml /etc/\ncp etc/systemd/system/containerd.service /etc/systemd/system/\ncp -r usr /\n\ncontainerd配置文件\n\nmkdir -p /etc/containerd\ncontainerd config default > /etc/containerd/config.toml\n# Options\nvi /etc/containerd/config.toml\n\n启动containerd\n\nsystemctl enable containerd\nsystemctl start containerd\nsystemctl status containerd\n\n配置kubelet\n\n\n准备配置文件\n\nmkdir -p /etc/kubernetes/ssl/\nmv ${HOSTNAME}-key.pem ${HOSTNAME}.pem ca.pem ca-key.pem /etc/kubernetes/ssl/\nmv ${HOSTNAME}.kubeconfig /etc/kubernetes/kubeconfig\nIP=192.168.200.22\n\ncat <<EOF > /etc/kubernetes/kubelet-config.yaml\nkind: KubeletConfiguration\napiVersion: kubelet.config.k8s.io/v1beta1\nauthentication:\n  anonymous:\n    enabled: false\n  webhook:\n    enabled: true\n  x509:\n    clientCAFile: \"/etc/kubernetes/ssl/ca.pem\"\nauthorization:\n  mode: Webhook\nclusterDomain: \"cluster.local\"\nclusterDNS:\n  - \"169.254.25.10\"\npodCIDR: \"10.200.0.0/16\"\naddress: ${IP}\nreadOnlyPort: 0\nstaticPodPath: /etc/kubernetes/manifests\nhealthzPort: 10248\nhealthzBindAddress: 127.0.0.1\nkubeletCgroups: /systemd/system.slice\nresolvConf: \"/etc/resolv.conf\"\nruntimeRequestTimeout: \"15m\"\nkubeReserved:\n  cpu: 200m\n  memory: 512M\ntlsCertFile: \"/etc/kubernetes/ssl/${HOSTNAME}.pem\"\ntlsPrivateKeyFile: \"/etc/kubernetes/ssl/${HOSTNAME}-key.pem\"\nEOF\n\n配置服务\n\ncat <<EOF > /etc/systemd/system/kubelet.service\n[Unit]\nDescription=Kubernetes Kubelet\nDocumentation=https://github.com/kubernetes/kubernetes\nAfter=containerd.service\nRequires=containerd.service\n\n[Service]\nExecStart=/usr/local/bin/kubelet \\\\\n  --config=/etc/kubernetes/kubelet-config.yaml \\\\\n  --container-runtime=remote \\\\\n  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\\n  --image-pull-progress-deadline=2m \\\\\n  --kubeconfig=/etc/kubernetes/kubeconfig \\\\\n  --network-plugin=cni \\\\\n  --node-ip=${IP} \\\\\n  --register-node=true \\\\\n  --v=2\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n配置nginx-proxy nginx-proxy是有一个用于Worker节点访问apiserver的一个代理， 是apiserver一个优雅的高可用方案。 它使用kubelet的staticpod方式启动，让每个节点都可以均衡的访问每个apiserver服务 优雅的替代了通过虚拟ip访问apiserver的方式。\n\n只需要在没有``apiserver的节点部署\n注意stream中有几个节点写几个\nmkdir -p /etc/nginx\nMASTER_IPS=(10.155.19.223 10.155.19.64)\n\ncat <<EOF > /etc/nginx/nginx.conf\nerror_log stderr notice;\n\nworker_processes 2;\nworker_rlimit_nofile 130048;\nworker_shutdown_timeout 10s;\n\nevents {\n  multi_accept on;\n  use epoll;\n  worker_connections 16384;\n}\n\nstream {\n  upstream kube_apiserver {\n    least_conn;\n    server ${MASTER_IPS[0]}:6443;\n    server ${MASTER_IPS[1]}:6443;\n    ...\n    server ${MASTER_IPS[N]}:6443;\n  }\n\n  server {\n    listen        127.0.0.1:6443;\n    proxy_pass    kube_apiserver;\n    proxy_timeout 10m;\n    proxy_connect_timeout 1s;\n  }\n}\n\nhttp {\n  aio threads;\n  aio_write on;\n  tcp_nopush on;\n  tcp_nodelay on;\n\n  keepalive_timeout 5m;\n  keepalive_requests 100;\n  reset_timedout_connection on;\n  server_tokens off;\n  autoindex off;\n\n  server {\n    listen 8081;\n    location /healthz {\n      access_log off;\n      return 200;\n    }\n    location /stub_status {\n      stub_status on;\n      access_log off;\n    }\n  }\n}\nEOF\n\nNginx manifest\n\ncat <<EOF > /etc/kubernetes/manifests/nginx-proxy.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-proxy\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    k8s-app: kube-nginx\nspec:\n  hostNetwork: true\n  dnsPolicy: ClusterFirstWithHostNet\n  nodeSelector:\n    kubernetes.io/os: linux\n  priorityClassName: system-node-critical\n  containers:\n  - name: nginx-proxy\n    image: docker.io/library/nginx:1.19\n    imagePullPolicy: IfNotPresent\n    resources:\n      requests:\n        cpu: 25m\n        memory: 32M\n    securityContext:\n      privileged: true\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8081\n    readinessProbe:\n      httpGet:\n        path: /healthz\n        port: 8081\n    volumeMounts:\n    - mountPath: /etc/nginx\n      name: etc-nginx\n      readOnly: true\n  volumes:\n  - name: etc-nginx\n    hostPath:\n      path: /etc/nginx\nEOF\n\n配置kube-proxy\n\n\n配置文件\n\nmv kube-proxy.kubeconfig /etc/kubernetes/\ncat <<EOF > /etc/kubernetes/kube-proxy-config.yaml\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nbindAddress: 0.0.0.0\nclientConnection:\n  kubeconfig: \"/etc/kubernetes/kube-proxy.kubeconfig\"\nclusterCIDR: \"10.200.0.0/16\"\nmode: ipvs\nEOF\n\nkube-proxy服务文件\n\ncat <<EOF > /etc/systemd/system/kube-proxy.service\n[Unit]\nDescription=Kubernetes Kube Proxy\nDocumentation=https://github.com/kubernetes/kubernetes\n\n[Service]\nExecStart=/usr/local/bin/kube-proxy \\\\\n  --config=/etc/kubernetes/kube-proxy-config.yaml\nRestart=on-failure\nRestartSec=5\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\n服务启动\n\nsystemctl daemon-reload\nsystemctl enable kubelet kube-proxy\nsystemctl start kubelet kube-proxy\njournalctl -f -u kubelet\njournalctl -f -u kube-proxy\n\n手动下载镜像pause(服务器无法访问外网) 在每个节点下载(master节点不下载可能不能成功)\n\ncrictl pull registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2\nctr -n k8s.io i tag  registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/pause:3.2 k8s.gcr.io/pause:3.2\n\n主动拉取/etc/kubernetes/manifests/nginx-proxy.yaml文件中的nginx镜像 具体版本，查看文件中的版本信息\n\ncrictl pull docker.io/library/nginx:1.19\n\n\n网络插件Calico\n\nYAML文件需要从官网下载\n\n\n链接: https://docs.projectcalico.org/getting-started/kubernetes/self-managed-onprem/onpremises\n有两种配置，50节点以内或者以上\n具体版本看官网信息，一下链接可能版本会不同\n\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/calico.yaml -O\n\n修改IP自动发现方式\n\n\nautodetect 可能会有问题\n当kubelet的启动参数中存在–node-ip的时候，以host-network模式启动的pod的status.hostIP字段就会自动填入kubelet中指定的ip地址\n\n修改前:\n- name: IP\n  value: \"autodetect\"\n修改后:\n- name: IP\n  valueFrom:\n    fieldRef:\n      fieldPath: status.hostIP\n\n修改CIDR\n\n修改前:\n# - name: CALICO_IPV4POOL_CIDR\n#   value: \"192.168.0.0/16\"\n修改后:\n- name: CALICO_IPV4POOL_CIDR\n  value: \"10.200.0.0/16\"\n\n启动calico\n\nkubectl apply -f calico.yaml\n\n\nDNS插件CoreDNS\n\nDeploy CoreDNS\n\n\n设置coredns的cluster-ip\n下载coredns.yaml\n替换cluster-ip\n创建coredns\n\nCOREDNS_CLUSTER_IP=10.233.0.10\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n      addonmanager.kubernetes.io/mode: EnsureExists\ndata:\n  Corefile: |\n    .:53 {\n        errors\n        health {\n            lameduck 5s\n        }\n        ready\n        kubernetes cluster.local in-addr.arpa ip6.arpa {\n          pods insecure\n          fallthrough in-addr.arpa ip6.arpa\n        }\n        prometheus :9153\n        forward . /etc/resolv.conf {\n          prefer_udp\n        }\n        cache 30\n        loop\n        reload\n        loadbalance\n    }\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: Reconcile\n  name: system:coredns\nrules:\n  - apiGroups:\n      - \"\"\n    resources:\n      - endpoints\n      - services\n      - pods\n      - namespaces\n    verbs:\n      - list\n      - watch\n  - apiGroups:\n      - \"\"\n    resources:\n      - nodes\n    verbs:\n      - get\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n    addonmanager.kubernetes.io/mode: EnsureExists\n  name: system:coredns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:coredns\nsubjects:\n  - kind: ServiceAccount\n    name: coredns\n    namespace: kube-system\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: coredns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    kubernetes.io/name: \"coredns\"\n    addonmanager.kubernetes.io/mode: Reconcile\n  annotations:\n    prometheus.io/port: \"9153\"\n    prometheus.io/scrape: \"true\"\nspec:\n  selector:\n    k8s-app: kube-dns\n  clusterIP: ${COREDNS_CLUSTER_IP}\n  ports:\n    - name: dns\n      port: 53\n      protocol: UDP\n    - name: dns-tcp\n      port: 53\n      protocol: TCP\n    - name: metrics\n      port: 9153\n      protocol: TCP\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: \"coredns\"\n  namespace: kube-system\n  labels:\n    k8s-app: \"kube-dns\"\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.io/name: \"coredns\"\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 10%\n  selector:\n    matchLabels:\n      k8s-app: kube-dns\n  template:\n    metadata:\n      labels:\n        k8s-app: kube-dns\n      annotations:\n        seccomp.security.alpha.kubernetes.io/pod: 'runtime/default'\n    spec:\n      priorityClassName: system-cluster-critical\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: coredns\n      tolerations:\n        - key: node-role.kubernetes.io/master\n          effect: NoSchedule\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - topologyKey: \"kubernetes.io/hostname\"\n            labelSelector:\n              matchLabels:\n                k8s-app: kube-dns\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            preference:\n              matchExpressions:\n              - key: node-role.kubernetes.io/master\n                operator: In\n                values:\n                - \"\"\n      containers:\n      - name: coredns\n        image: \"docker.io/coredns/coredns:1.6.7\"\n        imagePullPolicy: IfNotPresent\n        resources:\n          # TODO: Set memory limits when we've profiled the container for large\n          # clusters, then set request = limit to keep this container in\n          # guaranteed class. Currently, this container falls into the\n          # \"burstable\" category so the kubelet doesn't backoff from restarting it.\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-conf\", \"/etc/coredns/Corefile\" ]\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9153\n          name: metrics\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add:\n            - NET_BIND_SERVICE\n            drop:\n            - all\n          readOnlyRootFilesystem: true\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8181\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n      dnsPolicy: Default\n      volumes:\n        - name: config-volume\n          configMap:\n            name: coredns\n            items:\n            - key: Corefile\n              path: Corefile\n  ```\n\n```bash\nsed -i \"s/\\${COREDNS_CLUSTER_IP}/${COREDNS_CLUSTER_IP}/g\" coredns.yaml\nkubectl apply -f coredns.yaml\n\nDeploy NodeLocal DNSCache\n\n\n设置coredns的cluster-ip\n下载nodelocaldns.yaml\n替换`cluster-ip\n创建nodelocaldns\n\nCOREDNS_CLUSTER_IP=10.233.0.10\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: nodelocaldns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: EnsureExists\n\ndata:\n  Corefile: |\n    cluster.local:53 {\n        errors\n        cache {\n            success 9984 30\n            denial 9984 5\n        }\n        reload\n        loop\n        bind 169.254.25.10\n        forward . ${COREDNS_CLUSTER_IP} {\n            force_tcp\n        }\n        prometheus :9253\n        health 169.254.25.10:9254\n    }\n    in-addr.arpa:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind 169.254.25.10\n        forward . ${COREDNS_CLUSTER_IP} {\n            force_tcp\n        }\n        prometheus :9253\n    }\n    ip6.arpa:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind 169.254.25.10\n        forward . ${COREDNS_CLUSTER_IP} {\n            force_tcp\n        }\n        prometheus :9253\n    }\n    .:53 {\n        errors\n        cache 30\n        reload\n        loop\n        bind 169.254.25.10\n        forward . /etc/resolv.conf\n        prometheus :9253\n    }\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nodelocaldns\n  namespace: kube-system\n  labels:\n    k8s-app: kube-dns\n    addonmanager.kubernetes.io/mode: Reconcile\nspec:\n  selector:\n    matchLabels:\n      k8s-app: nodelocaldns\n  template:\n    metadata:\n      labels:\n        k8s-app: nodelocaldns\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9253'\n    spec:\n      priorityClassName: system-cluster-critical\n      serviceAccountName: nodelocaldns\n      hostNetwork: true\n      dnsPolicy: Default  # Don't use cluster DNS.\n      tolerations:\n      - effect: NoSchedule\n        operator: \"Exists\"\n      - effect: NoExecute\n        operator: \"Exists\"\n      containers:\n      - name: node-cache\n        image: \"registry.cn-hangzhou.aliyuncs.com/kubernetes-kubespray/dns_k8s-dns-node-cache:1.16.0\"\n        resources:\n          limits:\n            memory: 170Mi\n          requests:\n            cpu: 100m\n            memory: 70Mi\n        args: [ \"-localip\", \"169.254.25.10\", \"-conf\", \"/etc/coredns/Corefile\", \"-upstreamsvc\", \"coredns\" ]\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 53\n          name: dns\n          protocol: UDP\n        - containerPort: 53\n          name: dns-tcp\n          protocol: TCP\n        - containerPort: 9253\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 169.254.25.10\n            path: /health\n            port: 9254\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            host: 169.254.25.10\n            path: /health\n            port: 9254\n            scheme: HTTP\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/coredns\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n      volumes:\n        - name: config-volume\n          configMap:\n            name: nodelocaldns\n            items:\n            - key: Corefile\n              path: Corefile\n        - name: xtables-lock\n          hostPath:\n            path: /run/xtables.lock\n            type: FileOrCreate\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a \"force\n      # deletion\": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 20%\n    type: RollingUpdate\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: nodelocaldns\n  namespace: kube-system\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\nsed -i \"s/\\${COREDNS_CLUSTER_IP}/${COREDNS_CLUSTER_IP}/g\" nodelocaldns.yaml\nkubectl apply -f nodelocaldns.yaml\n\n\n集群冒烟测试\n\n创建nginx ds\n\ncat > nginx-ds.yml <<EOF\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ds\n  labels:\n    app: nginx-ds\nspec:\n  type: NodePort\n  selector:\n    app: nginx-ds\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: nginx-ds\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ds\n  template:\n    metadata:\n      labels:\n        app: nginx-ds\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx:1.19\n        ports:\n        - containerPort: 80\nEOF\n\nkubectl apply -f nginx-ds.yml\n\n检查各种IP连通性\n\n\n查看各Node上的Pod IP连通性\n\nkubectl get pods -o wide\n如下图 \n\n在每个Worker节点上ping pod ip\n\nping <pod-ip> #上图中查出来的\n\n查看service可达性\n\nkubectl get svc\n\n# 在每个`Worker`节点上访问服务\ncurl <service-ip>:<port> # 80\n\n在每个节点上检查`node-port`可用性\ncurl <node-ip>:<port> # node-ip: 192.168.200.22, port: 上图中80后面的端口号\n如下图: \n\n检查dns可用性\n\ncat > pod-nginx.yaml <<EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  containers:\n  - name: nginx\n    image: docker.io/library/nginx:1.19\n    ports:\n    - containerPort: 80\nEOF\n\n# 创建pod\n$ kubectl apply -f pod-nginx.yaml\n\n# 进入pod，查看dns(根据pod名)\n$ kubectl exec nginx -it -- /bin/bash\n\n# 查看dns配置(IP)\nroot@nginx:/# cat /etc/resolv.conf\n\n# 查看名字是否可以正确解析\nroot@nginx:/# curl nginx-ds"
  },
  {
    "objectID": "posts/poetry_pyright_doom_emacs/index.html",
    "href": "posts/poetry_pyright_doom_emacs/index.html",
    "title": "你好, Quarto",
    "section": "",
    "text": "Doom Emacs的python配置\n\n(python\n  +poetry\n  +pyright)\n\nInstall Poetry\n\npip install poetry\n\nInstall Pyright\n\nnpm install -g pyright\n\n\n\nmkdir project_name && cd project_name\npoetry init\npoetry install\npoetry add flask\nEmacs打开project_name/app.py\n这样Doom Emacs配置的Poetry的虚拟环境中的Flask,可以在python的LSP补全后端补全flask的代码了\n\n\n\n刚配置完成后，创建Poetry项目后可以正常激活，但是Pyright始终无法补全 最终在Reddit Link 中有人编辑器用了Github Repo上的一个项目后，然后 打开别的Poetry创建的项目，有人讨论说，可能是因为上面的Github仓库中多了poetry.toml文件 内容如下:\n[virtualenvs]\nin-project = true"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "曳影",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 23, 2022\n\n\n你好, Quarto\n\n\n曳影\n\n\n\n\nNov 19, 2022\n\n\nKubernetes the hard way\n\n\n曳影\n\n\n\n\nNov 15, 2022\n\n\n你好, Quarto\n\n\n曳影\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]